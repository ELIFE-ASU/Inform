= Shannon Information Measures

The `inform/shannon.h` header provides a collection of entropy and information
measures on discrete probability distributions
(link:index.html#inform_dist[inform_dist]). These functions provide the core of
*Inform* as all of the time series analysis functions are build upon them.

== Examples

=== Example 1: Entropy and Random Numbers
The `inform_shannon_entropy` function function allows us to calculate the
Shannon entropy of a distributions. Let’s try generating a random distribution
and see what the entropy looks like?
[source,c]
----
include::../../examples/shannon/entropy.c[]
----
This is exactly what you should expect; the pseudo-random number generate does a
decent job producing integers in a uniform fashion.

=== Example 2: Mutual Information
How correlated are consecutive integers? Let's give
link:index.html#inform_shannon_mi[mutual information] a try.
[source,c]
----
include::../../examples/shannon/mutual_information.c[]
----
Due to the subtlties of floating-point computation we don’t get zero. Really,
though the mutual information is zero.

=== Example 3: Relative Entropy and Biased Random Numbers
Okay. Now let’s generate some binary sequences. The first will be roughly
uniform, but the second will be biased toward `0`.
[source,c]
----
include::../../examples/shannon/relative_entropy.c[]
----

== API Documentation
