[[time-series-measures]]
= Time Series Measures

The original purpose of *Inform* was to analyze time series data. This explains why most of
*Inform*'s functionality resides in functions specifically optimized for analyzing time
series. The API was designed to be easy to use in C or {cpp}, or to be wrapped in a
higher-level language, e.g. https://elife-asu.github.io/PyInform[Python]. This means that we
avoided some of the "niceties" of C, such as extensive use of macros and other generic
atrocities, in favor of wrappability. Keep this in mind as you learn the API.

Many information measures have "local" variants which compute a time series of point-wise
values. These local variants have names similar to their averaged or global counterparts,
e.g <<inform_active_info,inform_active_info>> and
<<inform_local_active_info,inform_local_active_info>>. We have been meticulous in ensuring
that function and parameter names are consistent across measures. If you notice some
inconsistency, please https://github.com/elife-asu/inform/issue[report it as an issue].

[[time-series-notation]]
== Notation

Throughout the discussion of time series measures, we will try to use a consistent notation.
We will denote random variables as stem:[X,Y,\ldots], and let stem:[x_i,y_i,\ldots]
represent the stem:[i]-th time step of a time series drawn from the associated random
variable. Many of the measures consider stem:[k]-histories (a.k.a stem:[k]-blocks) of the
time series, e.g. stem:[x_i^{(k)} = \left\{x_{i-k+1}, x_{i-k+2},\ldots,x_i\right\}].

When denoting probability distributions, we will only make the random variable explicit in
situations where the notation is ambiguous. We will typically write stem:[p(x_i)],
stem:[p(x_i^{(k)})], and stem:[p(x_i^{(k)}, x_{i+1})] to denote the empirical probability
of observing the stem:[x_i] state, the stem:[x_i^{(k)}] stem:[k]-history, and the joint
probability of observing stem:[\left(x_i^{(k)}, x_{i+1}\right)].

*Please report any notational ambiguities as an
https://github.com/elife-asu/inform/issue[issue].*

[[time-series-detail]]
== Implementation Details

=== The Base: States and Logarithms
The word "base" has two different meanings in the context of information measures on time
series. It could refer to the base of the time series itself, that is the number of unique
states in the time series. For example, the time series stem:[\{0,2,1,0,0\}] is a base-3
time series. On the other hand, it could refer to the base of the logarithm used in
computing the information content of the inferred probability distributions. The problem is
that these two meanings clash. The base of the time series affects the range of values the
measure can produce, and the base of the logarithm represents a rescaling of those values.

In this library we deal with this by *always* using base-2 logarithms, and having the user
specify the base of the time series â€” don't worry, we <<error-handling, set an error>> if
the provided base doesn't make sense. All of this ensures that the library is a simple as
reasonably possible.

=== Multiple Initial Conditions
You generally need *a lot* of data to infer a probability distribution.  An experimentalist
or simulator might then collect data over multiple trials or initial conditions. Most of
*Inform*'s time series measures allow the user to pass in a two-dimensional, rectangular
array which each row representing a time series from a different initial condition. From
this the probability distributions are inferred, and the desired value is calculated. This
has the downside of requiring that the user store all of the data in memory, but it has the
advantage of being fast and simple. Trade-offs, man...

A subsequent release, https://github.com/elife-asu/inform/milestone/3[likely v1.1.0], will
allows the initial conditions to have time series of different lengths and will provide
accumulator implementations of all of these measures that will let the incrementally
construct the distributions. This will lift some of the memory burden at the expense of
runtime performance.

=== Calling Conventions
All of the of the time series functions described in this section use the same basic calling
conventions and use the same (or similar) argument names were possible.

|===
| Argument Name | Description

| `series`
| A 2-D or 3-D, finite-state time series in contiguous, row-major form

| `l`
| The number of "sources" or variables in a 3-D time series

| `n`
| The number of initial conditions per "source"

| `m`
| The number of time steps per "source"

| `b`
| The base of the time series

| `k`
| The length history length

| `err`
| An error argument
|===

Average measures generally return a double-precision, floating-point value while local
variants return a pointer to an appropriately shaped, contiguous array. Local measures
accept an argument, often named after the function (e.g. local active information takes an
`ai` argument), which is used to store the computed local values. If that argument is NULL,
then the function allocates an array.

We will try to note any deviations from these conventions.

[[active-info]]
== Active Information

Active information (AI) was introduced in <<Lizier2012>> to quantify information storage in
distributed computations. Active information is defined in terms of a temporally local
variant

[stem]
++++
a_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}}.
++++

where the probabilities are constructed empirically from the _entire_ time series. From the
local variant, the temporally global active information is defined as

[stem]
++++
A_X(k) = \langle a_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})p(x_{i+1})}}.
++++

Strictly speaking, the local and average active information are defined as

[stem]
++++
a_{X,i} = \lim_{k\rightarrow \infty}a_{X,i}(k)
\qquad \textrm{and} \qquad
A_X = \lim_{k\rightarrow \infty}A_X(k),
++++

but we do not provide limiting functionality in this library
(https://github.com/elife-asu/issues/24[yet]!).

****
[[inform_active_info]]
[source,c]
----
double inform_active_info(int const *series, size_t n, size_t m,
        int b, size_t k, inform_error *err);
----
Compute the average active information with a history length `k`.

*Examples:*

One initial condition:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[9] = {0,0,1,1,1,1,0,0,0};
double ai = inform_active_info(series, 1, 9, 2, 2, &err);
assert(inform_succeeded(&err));
// ai ~ 0.305958
----

Two initial conditions:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[18] = {0,0,1,1,1,1,0,0,0,
                        1,0,0,1,0,0,1,0,0};
double ai = inform_active_info(series, 2, 9, 2, 2, &err);
assert(inform_succeeded(&err));
// ai ~ 0.359879
----
[horizontal]
Header:: `inform/active_info.h`
****

****
[[inform_local_active_info]]
[source,c]
----
double *inform_local_active_info(int const *series, size_t n, size_t m,
        int b, size_t k, double *ai, inform_error *err);
----
Compute the local active information with a history length `k`.

*Examples:*

One initial condition:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[9] = {0,0,1,1,1,1,0,0,0};
double *ai = inform_local_active_info(series, 1, 9, 2, 2, NULL, &err);
assert(inform_succeeded(&err));
// ai ~ {-0.193, 0.807, 0.222, 0.222, -0.363, 1.222, 0.222}
free(ai);
----

Two initial conditions:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[18] = {0,0,1,1,1,1,0,0,0,
                        1,0,0,1,0,0,1,0,0};
double ai[14];
inform_local_active_info(series, 2, 9, 2, 2, ai, &err);
assert(inform_succeeded(&err));
// ai ~ { 0.807, -0.363, 0.637, 0.637, -0.778, 0.807, -1.193,
//        0.807,  0.807, 0.222, 0.807,  0.807, 0.222,  0.807 }

// no need to free since `ai` was statically allocated in this scope
// free(ai);
----
[horizontal]
Header:: `inform/active_info.h`
****

[[block-entropy]]
== Block Entropy
Block entropy, also known as stem:[N]-gram entropy <<Shannon1948>>, is the standard Shannon
entropy of the stem:[k]-histories of a time series:
[stem]
++++
H(X^{(k)}) = -\sum_{x_i^{(k)}} p(x_i^{(k)}) \log_2{p(x_i^{(k)})}
++++
which reduces to the traditional Shannon entropy for stem:[k=1].

****
[[inform_block_entropy]]
[source,c]
----
double inform_block_entropy(int const *series, size_t n, size_t m,
        int b, size_t k, inform_error *err);
----
Compute the average block entropy of a time series with block size `k`.

*Examples:*

One initial condition:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[9] = {0,0,1,1,1,1,0,0,0};

// k = 1
double h = inform_block_entropy(series, 1, 9, 2, 1, &err);
assert(inform_succeeded(&err));
// h ~ 0.991076

// k = 2
h = inform_block_entropy(series, 1, 9, 2, 2, &err);
assert(inform_succeeded(&err));
// h ~ 1.811278
----

Two initial conditions:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[18] = {0,0,1,1,1,1,0,0,0,
                        1,0,0,1,0,0,1,0,0};
double h = inform_active_info(series, 2, 9, 2, 2, &err);
assert(inform_succeeded(&err));
// h ~ 1.936278
----
[horizontal]
Header:: `inform/block_entropy.h`
****

****
[[inform_local_block_entropy]]
[source,c]
----
double *inform_local_block_entropy(int const *series, size_t n,
        size_t m, int b, size_t k, double *ent, inform_error *err);
----
Compute the local block entropy of a time series with block size `k`.

*Examples:*

One initial condition:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[9] = {0,0,1,1,1,1,0,0,0};

// k == 1
double *h = inform_local_block_entropy(series, 1, 9, 2, 1, NULL, &err);
assert(inform_succeeded(&err));
// h ~ { 0.848, 0.848, 1.170, 1.170, 1.170, 1.170, 0.848, 0.848, 0.848 }

// k == 2
double *h = inform_local_block_entropy(series, 1, 9, 2, 2, NULL, &err);
assert(inform_succeeded(&err));
// h ~ { 1.415, 3.000, 1.415, 1.415, 1.415, 3.000, 1.415, 1.415 }

free(ai);
----

Two initial conditions:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[18] = {0,0,1,1,1,1,0,0,0,
                        1,0,0,1,0,0,1,0,0};
double h[16];
inform_local_block_entropy(series, 2, 9, 2, 2, h, &err);
assert(inform_succeeded(&err));
// h ~ { 1.415, 2.415, 2.415, 2.415, 2.415, 2.000, 1.415, 1.415,
//       2.000, 1.415, 2.415, 2.000, 1.415, 2.415, 2.000, 1.415 }

// no need to free since `h` was statically allocated in this scope
// free(h);
----
[horizontal]
Header:: `inform/block_entropy.h`
****

[[conditional-entropy]]
== Conditional Entropy
https://en.wikipedia.org/wiki/Conditional_entropy[Conditional entropy] is a measure of the
amount of information required to describe a random variable stem:[Y] given knowledge of
another random variable stem:[X]. When applied to time series, two time series are used to
construct the empirical distributions, and <<inform_shannon_ce,inform_shannon_ce>> can be
applied to yield
[stem]
++++
H(Y|X) = - \sum_{x_i,y_i} p(x_i,y_i) \log_2{p(y_i|x_i)}.
++++
This can be viewed as the time-average of the local conditional entropy
[stem]
++++
h_i(Y|X) = -\log_2{p(y_i|x_i)}.
++++
See <<Cover1991>> for more information.

****
[[inform_conditional_entropy]]
[source,c]
----
double inform_conditional_entropy(int const *xs, int const *ys,
        size_t n, int bx, int by, inform_error *err);
----
Compute the conditional entropy between two time series.

This function expects the *condition* to be the first argument, `xs`. It is expected that
each time series be the same length `n`, but may have different bases `bx` and `by`.

*Examples:*
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[20] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1};
int const ys[20] = {0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1};

double ce = inform_conditional_entropy(xs, ys, 20, 2, 2, &err);
assert(inform_succeeded(&err));
// ce == 0.597107

ce = inform_conditional_entropy(ys, xs, 20, 2, 2, &err);
assert(inform_succeeded(&err));
// ce == 0.507757
----
[horizontal]
Header:: `inform/conditional_entropy.h`
****

****
[[inform_local_conditional_entropy]]
[source,c]
----
double *inform_local_conditional_entropy(int const *xs, int const *ys,
        size_t n, int bx, int by, double *mi, inform_error *err);
----
Compute the local conditional entropy between two time series.

This function expects the *condition* to be the first argument, `xs`. It is expected that
each time series be the same length `n`, but may have different bases `bx` and `by`.

*Examples:*
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[20] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1};
int const ys[20] = {0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1};

double *ce = inform_local_conditional_entropy(xs, ys, 20, 2, 2, NULL, &err);
assert(inform_succeeded(&err));
// ce == { 3.00, 3.00, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19,
//         0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.42, 0.42, 0.42, 2.00 }

inform_local_conditional_entropy(ys, xs, 20, 2, 2, ce, &err);
assert(inform_succeeded(&err));
// ce == { 1.32, 1.32, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10,
//         0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.74, 0.74, 0.74, 3.91 }

free(ce);
----
[horizontal]
Header:: `inform/conditional_entropy.h`
****

[[cross-entropy]]
== Cross Entropy
https://en.wikipedia.org/wiki/Cross_entropy[Cross entropy] between two distributions
stem:[p_X] and stem:[q_X] measures the amount of information needed to identify events
using a coding scheme optimized for stem:[q_X] when stem:[p_X] is the "real" distributions
over stem:[X].
[stem]
++++
H(p,q) = -\sum_{x} p(x) \log_2{q(x)}
++++
Cross entropy's local variant is equivalent to the self-information of stem:[q_X], and as
such is implemented by <<inform_local_block_entropy,inform_local_block_entropy>>.

See <<Cover1991>> for more details.
****
[[inform_cross_entropy]]
[source,c]
----
double inform_cross_entropy(int const *ps, int const *qs, size_t n,
        int b, inform_error *err);
----
Compute the cross entropy between the "true" and "unnatural" distributions stem:[p_X] and
stem:[q_X] from associated time series `ps` and `qs`, respectively.

*Examples:*
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const ps[10] = {0,1,1,0,1,0,0,1,0,0};
int const qs[10] = {0,0,0,0,0,1,1,0,0,1};

double ce = inform_cross_entropy(ps, qs, 10, 2, &err);
assert(inform_succeeded(&err));
// ce == 1.003530

ce = inform_cross_entropy(qs, ps, 10, 2, &err);
assert(inform_succeeded(&err));
// ce == 0.912454
----
[horizontal]
Header:: `inform/cross_entropy.h`
****

[[effective-information]]
== Effective Information

****
[[inform_effective_info]]
[source,c]
----
double inform_effective_info(double const *tpm, double const *inter,
        size_t n, inform_error *err);
----
[horizontal]
Header:: `inform/effective_info.h`
****

[[entropy-rate]]
== Entropy Rate
https://en.wikipedia.org/wiki/Entropy_rate[Entropy rate] quantifies the amount of
information needed to describe the next state of stem:[X] given observations of
stem:[X^{(k)}].  In other wrods, it is the entropy of the time series conditioned on the
stem:[k]-histories.  The local entropy rate
[stem]
++++
h_{X,i}(k) = \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
++++
can be averaged to obtain the global entropy rate
[stem]
++++
H_X(k) = \langle h_{X,i}(k) \rangle_i
       = \sum_{x_i^{(k)},x_{i+1}} p(x_i^{(k)},x_{i+1}) \log_2{\frac{p(x_i^{(k)}, x_{i+1})}{p(x_i^{(k)})}}.
++++
Much as with <<active-info, active information>>, the local and average entropy rates are
formally obtained in the limit
[stem]
++++
h_{X,i} = \lim_{k\rightarrow \infty}h_{X,i}(k)
\qquad \textrm{and} \qquad
H_X = \lim_{k\rightarrow \infty}H_X(k),
++++

but we do not provide limiting functionality in this library
(https://github.com/elife-asu/issues/24[yet]!).

See <<Cover1991>> for more details.

****
[[inform_entropy_rate]]
[source,c]
----
double inform_entropy_rate(int const *series, size_t n, size_t m,
        int b, size_t k, inform_error *err);
----
Compute the average entropy rate with a history length `k`.

*Examples:*

One initial condition:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[9] = {0,0,1,1,1,1,0,0,0};
double er = inform_entropy_rate(series, 1, 9, 2, 2, &err);
assert(inform_succeeded(&err));
// er ~ 0.679270
----

Two initial conditions:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[18] = {0,0,1,1,1,1,0,0,0,
                        1,0,0,1,0,0,1,0,0};
double er = inform_entropy_rate(series, 2, 9, 2, 2, &err);
assert(inform_succeeded(&err));
// er ~ 0.625349
----
[horizontal]
Header:: `inform/entropy_rate.h`
****

****
[[inform_local_entropy_rate]]
[source,c]
----
double *inform_local_entropy_rate(int const *series, size_t n,
        size_t m, int b, size_t k, double *er, inform_error *err);
----
Compute the local entropy rate with a history length `k`.

*Examples:*

One initial condition:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[9] = {0,0,1,1,1,1,0,0,0};
double *er = inform_local_entropy_rate(series, 1, 9, 2, 2, NULL, &err);
assert(inform_succeeded(&err));
// er ~ { 1.000, 0.000, 0.585, 0.585, 1.585, 0.000, 1.000 }
free(er);
----

Two initial conditions:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const series[18] = {0,0,1,1,1,1,0,0,0,
                        1,0,0,1,0,0,1,0,0};
double er[14];
inform_local_entropy_rate(series, 2, 9, 2, 2, er, &err);
assert(inform_succeeded(&err));
// er ~ { 0.415, 1.585, 0.585, 0.585, 1.585, 0.000, 2.000,
//        0.000, 0.415, 0.585, 0.000, 0.415, 0.585, 0.000 }

// no need to free since `er` was statically allocated in this scope
// free(er);
----
[horizontal]
Header:: `inform/entropy_rate.h`
****

[[excess-entropy]]
== Excess Entropy

****
[[inform_excess_entropy]]
[source,c]
----
double inform_excess_entropy(int const *series, size_t n, size_t m,
        int b, size_t k, inform_error *err);
----
[horizontal]
Header:: `inform/excess_entropy.h`
****

****
[[inform_local_excess_entropy]]
[source,c]
----
double *inform_local_excess_entropy(int const *series, size_t n,
        size_t m, int b, size_t k, double *ee, inform_error *err);
----
[horizontal]
Header:: `inform/excess_entropy.h`
****

[[information-flow]]
== Information Flow

****
[[inform_information_flow]]
[source,c]
----
double inform_information_flow(int const *src, int const *dst,
        int const *back, size_t l_src, size_t l_dst, size_t l_back,
        size_t n, size_t m, int b, inform_error *err);
----
[horizontal]
Header:: `inform/information_flow.h`
****

[[evidence-of-integration]]
== Evidence Of Integration

****
[[inform_integration_evidence]]
[source,c]
----
double *inform_integration_evidence(int const *series, size_t l,
        size_t n, int const *b, double *evidence, inform_error *err);
----
[horizontal]
Header:: `inform/information_flow.h`
****

****
[[inform_integration_evidence_part]]
[source,c]
----
double *inform_integration_evidence_part(int const *series, size_t l,
        size_t n, int const *b, size_t const *parts, size_t nparts,
        double *evidence, inform_error *err);
----
[horizontal]
Header:: `inform/information_flow.h`
****

[[mutual-information]]
== Mutual Information
https://en.wikipedia.org/wiki/Mutual_information[Mutual information] (MI) is a measure of
the amount of mutual dependence between at least two random variables. Locally, MI is
defined as
[stem]
++++
i_i(X_1,\ldots,X_l) = \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
++++
The mutual information is then just the time average of stem:[i_i(X_1,\ldots,X_l)]:
[stem]
++++
I(X_1,\ldots,X_l) =
    \sum_{x_{1,i},\ldots,x_{l,i}} p(x_{1,i},\ldots,x_{l,i}) \frac{p(x_{1,i},\ldots,x_{l,i})}{p(x_{1,i})\ldots p(x_{l,i})}.
++++
See <<Cover1991>> for more details.

****
[[inform_mutual_info]]
[source,c]
----
double inform_mutual_info(int const *series, size_t l, size_t n,
        int const *b, inform_error *err);
----
Compute the mutual information between two or more time series.

For this function, `l` is the number of random variables, and `n` is the length of each
variable's time series. Each variable can have a different base, so `b` is an array of
length `l`.

*Examples:*

Two variables:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[40] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1, // var 1
                    0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1  // var 2};

double mi = inform_mutual_info(xs, 2, 20, (int[2]){2,2}, &err);
assert(inform_succeeded(&err));
// mi == 0.214171
----

Three variables:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[60] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1, // var 1
                    0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1, // var 2
                    1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1, // var 3};

double mi = inform_mutual_info(xs, 3, 20, (int[3]){2,2,2}, &err);
assert(inform_succeeded(&err));
// mi == 1.095462
----
[horizontal]
Header:: `inform/mutual_info.h`
****

****
[[inform_local_mutual_info]]
[source,c]
----
double *inform_local_mutual_info(int const *series, size_t l, size_t n,
        int const *b, double *mi, inform_error *err);
----
Compute the local mutual information between two or more time series.

For this function, `l` is the number of random variables, and `n` is the length of each
variable's time series. Each variable can have a different base, so `b` is an array of
length `l`.

*Examples:*

Two variables:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[40] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1, // var 1
                    0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1  // var 2};

double *mi = inform_local_mutual_info(xs, 2, 20, (int[2]){2,2}, NULL, &err);
assert(inform_succeeded(&err));
// mi ~ { -1.000, -1.000, 0.222,  0.222, 0.222, 0.222, 0.222, 0.222,
//         0.222,  0.222, 0.222,  0.222, 0.222, 0.222, 0.222, 0.222,
//         1.585,  1.585, 1.585, -1.585 }
free(mi);
----

Three variables:
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[60] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1, // var 1
                    0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1, // var 2
                    1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1, // var 3};


double *mi = inform_local_mutual_info(xs, 3, 20, (int[3]){2,2,2}, NULL, &err);
assert(inform_succeeded(&err));
// mi ~ { 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737,
//        0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737,
//        3.322, 3.322, 0.322, 0.152 }
free(mi);
----
[horizontal]
Header:: `inform/mutual_info.h`
****

[[partial-information-decomposition]]
== Partial Information Decomposition

****
[[inform_pid_source]]
[source,c]
----
typedef struct inform_pid_source
{
    size_t *name;
    struct inform_pid_source **above;
    struct inform_pid_source **below;
    size_t size, n_above, n_below;
    double imin;
    double pi;
} inform_pid_source;
----
[horizontal]
Header:: `inform/pid.h`
****

****
[[inform_pid_lattice]]
[source,c]
----
typedef struct inform_pid_lattice
{
    inform_pid_source **sources;
    inform_pid_source *top;
    inform_pid_source *bottom;
    size_t size;
} inform_pid_lattice;
----
[horizontal]
Header:: `inform/pid.h`
****

****
[[inform_pid_lattice_free]]
[source,c]
----
void inform_pid_lattice_free(inform_pid_lattice *l);
----
[horizontal]
Header:: `inform/pid.h`
****

****
[[inform_pid]]
[source,c]
----
inform_pid_lattice *inform_pid(int const *stimulus,
        int const *responses, size_t l, size_t n, int bs,
        int const *br, inform_error *err);
----
[horizontal]
Header:: `inform/pid.h`
****

[[predictive-information]]
== Predictive Information

****
[[inform_predictive_info]]
[source,c]
----
double inform_predictive_info(int const *series, size_t n, size_t m,
        int b, size_t kpast, size_t kfuture, inform_error *err);
----
[horizontal]
Header:: `inform/predictive_info.h`
****

****
[[inform_local_predictive_info]]
[source,c]
----
double *inform_local_predictive_info(int const *series, size_t n,
        size_t m, int b, size_t kpast, size_t kfuture, double *pi,
        inform_error *err);
----
[horizontal]
Header:: `inform/predictive_info.h`
****

[[relative-entropy]]
== Relative Entropy
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence[Relative entropy], also
known as the Kullback-Leibler divergence, measures the amount of information gained in
switching from a prior distribution stem:[q_X] to a posterior distribution stem:[p_X] over
_the same support_:
[stem]
++++
D_{KL}(p||q) = \sum_{x_i} p(x_i) \log_2{\frac{p(x_i)}{q(x_i)}}.
++++
The local counterpart is
[stem]
++++
d_{KL,i}(p||q) = log_2{\frac{p(x_i)}{q(x_i)}}.
++++
Note that the average in moving from the local to the non-local relative entropy is taken
over the posterior distribution.

See <<Kullback1951>> and <<Cover1991>> for more information.

****
[[inform_relative_entropy]]
[source,c]
----
double inform_relative_entropy(int const *xs, int const *ys, size_t n,
        int b, inform_error *err);
----
Compute the relative entropy between time series drawn from posterior and prior
distributions, here `xs` and `ys` respectively.

*Examples:*
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[10] = {0,1,0,0,0,0,0,0,0,1};
int const ys[10] = {0,1,1,1,1,0,0,1,0,0};

double re = inform_relative_entropy(xs, ys, 10, 2, &err);
assert(inform_succeeded(&err));
// re == 0.278072

re = inform_relative_entropy(ys, xs, 10, 2, &err);
assert(inform_succeeded(&err));
// re == 0.321928
----
[horizontal]
Header:: `inform/relative_entropy.h`
****

****
[[inform_local_relative_entropy]]
[source,c]
----
double *inform_local_relative_entropy(int const *xs, int const *ys,
        size_t n, int b, double *re, inform_error *err);
----

*Examples:*
[source,c]
----
inform_error err = INFORM_SUCCESS;
int const xs[10] = {0,1,0,0,0,0,0,0,0,1};
int const ys[10] = {0,1,1,1,1,0,0,1,0,0};

double *re = inform_local_relative_entropy(xs, ys, 10, 2, NULL, &err);
assert(inform_succeeded(&err));
// re ~ { 0.678, -1.322, 0.678, 0.678, 0.678, 0.678, 0.678,
//        0.678, 0.678, -1.322 };

inform_local_relative_entropy(ys, xs, 10, 2, re, &err);
assert(inform_succeeded(&err));
// re ~ { -0.678, 1.322, 1.322, 1.322, 1.322, -0.678, -0.678, 1.322,
//        -0.678, -0.678 }

free(re);
----
[horizontal]
Header:: `inform/relative_entropy.h`
****

[[separable-information]]
== Separable Information

****
[[inform_separable_info]]
[source,c]
----
double inform_separable_info(int const *srcs, int const *dest,
        size_t l, size_t n, size_t m, int b, size_t k,
        inform_error *err);
----
[horizontal]
Header:: `inform/separable_info.h`
****

****
[[inform_local_separable_info]]
[source,c]
----
double *inform_local_separable_info(int const *srcs, int const *dest,
        size_t l, size_t n, size_t m, int b, size_t k, double *si,
        inform_error *err);
----
[horizontal]
Header:: `inform/separable_info.h`
****

[[transfer-entropy]]
== Transfer Entropy

****
[[inform_transfer_entropy]]
[source,c]
----
double inform_transfer_entropy(int const *src, int const *dst,
        int const *back, size_t l, size_t n, size_t m, int b, size_t k,
        inform_error *err);
----
[horizontal]
Header:: `inform/transfer_entropy.h`
****

****
[[inform_local_transfer_entropy]]
[source,c]
----
double *inform_local_transfer_entropy(int const *src, int const *dst,
        int const *back, size_t l, size_t n, size_t m, int b, size_t k,
        double *te, inform_error *err);
----
[horizontal]
Header:: `inform/transfer_entropy.h`
****
